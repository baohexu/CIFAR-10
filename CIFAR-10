import jax
import jax.numpy as jnp
from flax import nnx
import optax
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pickle
import urllib.request
import tarfile
import os

BATCH_SIZE = 64
LEARNING_RATE = 3e-4
EPOCHS = 10
DEVICE = jax.devices()[0]

class CIFAR10Dataset(Dataset):
    def __init__(self, root="./data", train=True):
        self.root = root
        self.train = train
        self.download()
        self.data, self.targets = self.load_data()
    
    def download(self):
        url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
        filepath = os.path.join(self.root, "cifar-10-python.tar.gz")
        
        if not os.path.exists(filepath):
            os.makedirs(self.root, exist_ok=True)
            urllib.request.urlretrieve(url, filepath)
            
        extract_path = os.path.join(self.root, "cifar-10-batches-py")
        if not os.path.exists(extract_path):
            with tarfile.open(filepath, 'r:gz') as tar:
                tar.extractall(self.root)
    
    def load_data(self):
        path = os.path.join(self.root, "cifar-10-batches-py")
        files = [f"data_batch_{i}" for i in range(1, 6)] if self.train else ["test_batch"]
        
        data, labels = [], []
        for file in files:
            with open(os.path.join(path, file), 'rb') as f:
                entry = pickle.load(f, encoding='latin1')
                data.append(entry['data'])
                labels.extend(entry['labels'])
        
        data = np.vstack(data).reshape(-1, 3, 32, 32)
        return data, np.array(labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        img = self.data[idx].astype(np.float32) / 255.0
        mean = np.array([0.4914, 0.4822, 0.4465]).reshape(3, 1, 1)
        std = np.array([0.2023, 0.1994, 0.2010]).reshape(3, 1, 1)
        img = (img - mean) / std
        return img, self.targets[idx]

train_loader = DataLoader(CIFAR10Dataset(train=True), batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(CIFAR10Dataset(train=False), batch_size=BATCH_SIZE)

class SimpleCNN(nnx.Module):
    def __init__(self, rngs: nnx.Rngs):
        super().__init__()
        self.conv1 = nnx.Conv(3, 32, kernel_size=(3, 3), padding="SAME", rngs=rngs)
        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), padding="SAME", rngs=rngs)
        self.fc1 = nnx.Linear(64 * 8 * 8, 256, rngs=rngs)
        self.fc2 = nnx.Linear(256, 10, rngs=rngs)
    
    def __call__(self, x):
        x = self.conv1(x)
        x = nnx.relu(x)
        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = self.conv2(x)
        x = nnx.relu(x)
        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = x.reshape(x.shape[0], -1)
        x = self.fc1(x)
        x = nnx.relu(x)
        x = self.fc2(x)
        return x

def compute_loss(logits, labels):
    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()

@nnx.jit
def train_step(graphdef, params, opt_state, batch):
    images, labels = batch
    
    def loss_fn(params):
        model = nnx.merge(graphdef, params)
        logits = model(images)
        return compute_loss(logits, labels)
    
    loss, grads = jax.value_and_grad(loss_fn)(params)
    updates, opt_state = optimizer.update(grads, opt_state, params)
    params = optax.apply_updates(params, updates)
    return graphdef, params, opt_state, loss

@nnx.jit
def eval_step(graphdef, params, batch):
    images, labels = batch
    model = nnx.merge(graphdef, params)
    logits = model(images)
    predictions = jnp.argmax(logits, axis=1)
    accuracy = jnp.mean(predictions == labels)
    return accuracy

def prepare_batch(batch):
    images = jnp.array(batch[0].numpy())
    images = jnp.transpose(images, (0, 2, 3, 1))
    labels = jnp.array(batch[1].numpy(), dtype=jnp.int32)
    return images, labels

def main():
    rngs = nnx.Rngs(42)
    model = SimpleCNN(rngs=rngs)
    graphdef, params = nnx.split(model)
    
    global optimizer
    optimizer = optax.adam(LEARNING_RATE)
    opt_state = optimizer.init(params)
    
    for epoch in range(EPOCHS):
        train_losses = []
        for batch_idx, batch in enumerate(train_loader):
            images, labels = prepare_batch(batch)
            images = jax.device_put(images, DEVICE)
            labels = jax.device_put(labels, DEVICE)
            graphdef, params, opt_state, loss = train_step(
                graphdef, params, opt_state, (images, labels)
            )
            train_losses.append(loss)
            if batch_idx % 100 == 0:
                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss:.4f}')
        
        accuracies = []
        for batch in test_loader:
            images, labels = prepare_batch(batch)
            images = jax.device_put(images, DEVICE)
            labels = jax.device_put(labels, DEVICE)
            accuracy = eval_step(graphdef, params, (images, labels))
            accuracies.append(accuracy)
        
        avg_accuracy = np.mean(accuracies)
        print(f'Epoch {epoch+1}, Accuracy: {avg_accuracy:.4f}')
        
        if avg_accuracy > 0.5:
            print("成功")
            return True
    
    final_accuracy = np.mean(accuracies)
    print(f"最终准确率: {final_accuracy:.4f}")
    
    if final_accuracy > 0.5:
        print("成功")
        return True
    else:
        print("失败")
        return False

if __name__ == "__main__":
    main()  
